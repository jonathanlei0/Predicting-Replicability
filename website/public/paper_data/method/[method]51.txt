<h3>Experiment 1</h3><br/><p class="body-paragraph" data-auto="body_paragraph">In the first
experiment, participants were presented with green pictures superimposed onto red
pictures. <strong data-auto="strong_text">Eye</strong> <strong data-auto="strong_text">movements</strong> were recorded while participants named the green picture of
the picture–picture stimuli (presented on the left side of a computer
screen) and manually responded by pressing a left or right button in response to the
left- or right-pointing arrows (&lt; or &gt;, presented on the right side of
the screen). For example, they said “hammer” in response to a
pictured hammer in green color, while trying to ignore a pictured chisel in red (the
semantically related condition) or a pictured sandal in red (the semantically
unrelated condition). Or they said “circle” in response to a
pictured circle in green, while trying to ignore a pictured circus in red (the
phonologically related condition) or a pictured table in red (the phonologically
unrelated condition). To minimize the chance that participants would identify the
direction of the arrows by their peripheral vision, the arrows were flanked by two
Xs on each side, yielding <em>XX &lt;XX</em> and <em>XX&gt;
XX</em> as stimuli.</p> <h4>Method</h4><p class="body-paragraph" data-auto="body_paragraph"><strong>Participants</strong></p><p class="body-paragraph" data-auto="body_paragraph">The
experiment was carried out with a group of 24 paid participants from the
pool at the Max Planck Institute for Psycholinguistics in Nijmegen. All
participants were young adults who were native speakers of Dutch. None of
the participants took part in one of the other experiments.</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Materials and design</strong></p><p class="body-paragraph" data-auto="body_paragraph">From the
picture gallery available at the Max Planck Institute, 40 pictured objects
were selected. All pictures had disyllabic names. The pictures were chosen
such that 10 pairs of pictures had names that were semantically related and
the 10 remaining pairs had names that were phonologically related. The
pictures with phonologically related names shared the first syllable. The
unrelated conditions were created by recombining the pictures such that each
semantically related picture also served as a semantically unrelated picture
and each phonologically related picture also served as a phonologically
unrelated picture. The Appendix lists the materials. The pictures were line
drawings on a black background. They were digitized and scaled to fit into a
virtual frame of 10 cm × 10 cm. On average, the pictures subtended
8.7° horizontally and 8.7° vertically at a viewing
distance of 66 cm (roughly the distance between the participant and the
screen). The arrows were presented in 28-point uppercase Arial font,
subtending 3.5° horizontally and 0.9° vertically. The
horizontal distance between the middle of the picture–picture
stimuli and the arrow stimuli was 15.2°.</p><p class="body-paragraph" data-auto="body_paragraph">There were
two independent variables: type and relation. The variable type indicated
whether the pictures were from the semantic or the phonological sets. The
variable relation indicated whether the paired pictures were related or
unrelated. Both variables were tested within participants. Relation was
tested within items and type was tested between items. A participant
received 20 picture–picture pairings in each of the four
distractor conditions, yielding 80 picture–picture stimuli in
total. Each picture pair was presented twice, yielding 160 trials per
participant in total. The order of presenting the stimuli across trials was
random, except that repetitions of pictures and <strong data-auto="strong_text">words</strong> on successive trials
were not permitted.</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Apparatus</strong></p><p class="body-paragraph" data-auto="body_paragraph">Materials
were presented on a 39-cm ViewSonic 17PS screen. <strong data-auto="strong_text">Eye</strong> <strong data-auto="strong_text">movements</strong> were measured
<strong data-auto="strong_text">using</strong> an SMI EyeLink-HiSpeed 2D headband-mounted <strong data-auto="strong_text">eye</strong>-tracking system
(SensoMotoric Instruments GmbH, Teltow, Germany). The <strong data-auto="strong_text">eye</strong> tracker was
controlled by a Pentium 90 MHz computer. The experiment was run under the
Nijmegen Experiment Setup (NESU) with a NESU button box on a Pentium 400 MHz
computer. The participants' utterances were recorded over a Sennheiser ME400
microphone to a SONY DTC55 digital audio tape (DAT) recorder. Vocal response
latencies were measured <strong data-auto="strong_text">using</strong> an electronic voice key.</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Procedure</strong></p><p class="body-paragraph" data-auto="body_paragraph">The
participants were tested individually. They were seated in front of the
computer monitor, a panel with a left and a right push button, and the
microphone. The distance between participant and screen was approximately 66
cm. Participants were given written instructions telling them how their <strong data-auto="strong_text">eyes</strong>
would be monitored and what the task was. The experimenter also orally
described the <strong data-auto="strong_text">eye</strong>-tracking equipment and restated the instructions. The
participants were told that they had to name the green picture of
picture–picture stimuli presented on the left side of a computer
screen and manually respond by pressing a left or right button in response
to the arrows presented on the right side of the screen. To familiarize them
with the pictures, the participants received a booklet showing them all
pictures used in the experiment together with the expected names.</p><p class="body-paragraph" data-auto="body_paragraph">When a
participant had read the instructions and studied the picture booklet, the
headband of the <strong data-auto="strong_text">eye</strong>-tracking system was placed on the participant's head,
and the system was calibrated and validated. For pupil-to-gaze calibration,
a grid of 3 × 3 positions had been defined. During a calibration
trial, a fixation target appeared once, in random order, in each of these
positions for one second. Participants were asked to fixate upon each target
until the next target appeared. After the calibration trial, the estimated
positions of the participant's fixations and the distances from the fixation
targets were displayed to the experimenter. Calibration was considered
adequate if there was at least one fixation within 1.5° of each
fixation target. When calibration was inadequate, the procedure was
repeated, sometimes after adjusting the <strong data-auto="strong_text">eye</strong> cameras. Successful calibration
was followed by a pupil-to-gaze validation trial. For the participants, this
trial did not differ from the calibration trial, but the data collected
during the validation trial were used to estimate the participants' gaze
positions, and the error (i.e., the distance between the estimated gaze
position and the target position) was measured. Validation was considered
completed if the average error was less than 1.0° and the worst
error less than 1.5°. Depending on the result of the validation
trial, the calibration and validation trials were repeated or testing began.</p><p class="body-paragraph" data-auto="body_paragraph">After
successful calibration and validation, a block of 40 practice trials was
administered, in which each picture was shown and named once. This was
followed by the 160 <strong data-auto="strong_text">experimental</strong> trials. The structure of a trial was as
follows. A trial started by the simultaneous presentation of the left
picture–picture and right arrow stimuli. The stimuli remained on
the screen until the participant pushed the button in response to the arrow.
The arrows were presented in white. The colored pictures and the arrows were
presented on a black background. Before the start of the next trial there
was a blank interval of 1.5 sec. The position of the left and right <strong data-auto="strong_text">eyes</strong> was
determined every 4 ms. Drift correction occurred automatically after every 8
trials.</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Analyses</strong></p><p class="body-paragraph" data-auto="body_paragraph">A naming
response was considered to be invalid when it included a speech error, when
a wrong <strong data-auto="strong_text">word</strong> was produced, or when the voice key was triggered incorrectly.
Error trials were discarded from the analyses of the naming latencies and
gaze shift latencies. To analyze the speakers' gaze shifts, their <strong data-auto="strong_text">eye</strong>
fixations were classified as falling within or on the outer contours of the
left stimulus or elsewhere. Although viewing was binocular and the positions
of both <strong data-auto="strong_text">eyes</strong> were tracked, only the position of the right <strong data-auto="strong_text">eye</strong> was analyzed.
Gaze shift latency was defined as the time interval between the beginning of
the first fixation on the left stimulus and the end of the last fixation
before the first shift of gaze was initiated to the right arrow. The vocal
response latencies, gaze shift latencies, and errors were submitted to
analyses of variance. The analyses were performed both by participants
(<em>F</em>1) and
by items (<em>F</em>2).
Faster responding in the related than unrelated condition will be called
descriptively <em>facilitation</em> and slower responding will be
called <em>interference</em>.</p> 